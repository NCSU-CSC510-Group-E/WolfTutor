\subsubsection{In Person Evaluations}

\paragraph{Survey Design}
Evaluations were completed in person with 10 different volunteer students over 2 days.
The goal was to check on the ease of use of the new WolfTutor application; to make sure
the enhancements did not make the application any harder to use or slower in live-time. 
To determine how long a user would spend finding a tutor on the application, two missions were given
to the students. One, to find a tutor as a student who really wanted a tutor with a high GPA. Two, to find a tutor
as a student that really wanted a tutor with good reviews. Students were encouraged to take other factors into account
as well to make the simulation more realistic. To avoid bias, a coin was flipped to decide which WolfTutor system to start with - old or new.
This resulted in 6 users evaluating the old system before the new system. 

\paragraph{Results}
Out of the 10 students, 4 took longer to use the new WolfTutor application when searching for GPA. The largest time
increase was 31 seconds. When searching for a tutor with good reviews, a different set of 4 students took longer with the new
WolfTutor. The longest time increase was 56 seconds. Even though more users completed the time trials
for the old application first, the users that completed the new application's trials first only accounted for 4 of the 
12 time differences that resulted in the new application being faster.

Students were asked to provide the tutor that they chose during the time trials.
When filtering by GPA, there were a total of 8 different tutors chosen in the old appliaction and
only 3 in the new one. When filtering by reviews, 8 different tutors were chosen in the old and 5 were
chosen in the new. 

Students were also asked to provide a brief reasoning for why they chose their tutor.
For the new application and for the review filter on the old application, the responses vary between the qualities of a tutor
the student prioritizes. For example: major, level of degree, and price.  When filtering for GPA, some students still took reviews
into account because it was important to them. Not just having a high average rating, but having a better distribution of ratings such that there
were more high ratings. Since there is no GPA given on the old system, students were asked to try to pick a tutor 
based on whether they seemed like they would have a high GPA or that seemed to be the smartest. For the most part, the reasoning given
for choosing a tutor was random selection, although some students did take other qualities into account.
For example, choosing the tutor that had the highest degree level that they could find.

These results signify: 
\begin{itemize}
  \item that the new WolfTutor application does not break the system such that it lessens usability.
  \item the qualities that make a good tutor are controverisal and the application could use more filters
  \item the new WolfTutor helped make the better tutors more obvious, as the range of tutors chosen decreased.
\end{itemize}


\paragraph{Feedback}
At the beginning of the evaluation, students were asked if they had ever paid for a tutor in the past. Out of all 10 students,
6 had and 4 had not. Out of the 6 that had, 2 of them reported finding the tutors online. The other
4 either found their tutor through a friend's recommendation or the tutor was actually their friend. Since most
of them never used an online service for finding a tutor, there did not seem to be much bias. It also meant
the students did not have a better system in mind to offer additonal features that could be implemented in WolfTutor.
However, some students did make suggestions:
\begin{itemize}
  \item adding in a filter for price
  \item being able to demo with a tutor
  \item the ability to change the weights of the filters because they did not think GPA was that important
  \item a larger selection of tutors. This is addressed in section \ref{mock-data}
\end{itemize}



\subsubsection{Responding to Change}
\label{sec:change}
\input{sections/sec_evaluation_change.tex}